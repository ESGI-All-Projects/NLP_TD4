{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Initial setup\n",
    "from src.ner_utils import load_data_from_csv, create_dataset, tokenize_and_align_labels, train_model, save_model, extract_entities\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForTokenClassification\n",
    "\n",
    "# Define necessary variables\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0b6d677d25b70f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Load and prepare data\n",
    "file_path = \"data/train_2.csv\" # Update this path according to your dataset\n",
    "df = load_data_from_csv(file_path)\n",
    "data = create_dataset(df)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35ea44c53ba9c25f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize and align labels\n",
    "tokenized_data = data.map(tokenize_and_align_labels, batched=True)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c848653f6841fda9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "num_labels = len(set([label for sublist in df['labels'] for label in sublist])) + 1 # Adjust based on your label set\n",
    "model = TFDistilBertForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5873f29c185dbd88"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1331f45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T15:57:31.916757700Z",
     "start_time": "2024-02-12T15:57:30.516241900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\programme\\anaconda3\\envs\\NLP_TD4\\lib\\site-packages\\transformers\\trainer_tf.py:118: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "{{function_node __wrapped__CreateSummaryFileWriter_device_/job:localhost/replica:0/task:0/device:CPU:0}} D:\\Documents\\école\\NLP_TD4\\logs is not a directory [Op:CreateSummaryFileWriter] name: ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFailedPreconditionError\u001B[0m                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenized_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Documents\\école\\NLP_TD4\\src\\ner_utils.py:121\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(tokenized_data, model, epochs, batch_size)\u001B[0m\n\u001B[0;32m    108\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(logging_dir, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    110\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TFTrainingArguments(\n\u001B[0;32m    111\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mD:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDocuments\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mécole\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mNLP_TD4\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mresults\u001B[39m\u001B[38;5;124m\"\u001B[39m,  \u001B[38;5;66;03m# Use the absolute path\u001B[39;00m\n\u001B[0;32m    112\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39mepochs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    118\u001B[0m     logging_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,\n\u001B[0;32m    119\u001B[0m )\n\u001B[1;32m--> 121\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTFTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenized_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    127\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32mD:\\Documents\\programme\\anaconda3\\envs\\NLP_TD4\\lib\\site-packages\\transformers\\trainer_tf.py:129\u001B[0m, in \u001B[0;36mTFTrainer.__init__\u001B[1;34m(self, model, args, train_dataset, eval_dataset, compute_metrics, tb_writer, optimizers)\u001B[0m\n\u001B[0;32m    127\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtb_writer \u001B[38;5;241m=\u001B[39m tb_writer\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 129\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtb_writer \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msummary\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_file_writer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogging_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_wandb_available():\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetup_wandb()\n",
      "File \u001B[1;32mD:\\Documents\\programme\\anaconda3\\envs\\NLP_TD4\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:583\u001B[0m, in \u001B[0;36mcreate_file_writer_v2\u001B[1;34m(logdir, max_queue, flush_millis, filename_suffix, name, experimental_trackable, experimental_mesh)\u001B[0m\n\u001B[0;32m    579\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _TrackableResourceSummaryWriter(\n\u001B[0;32m    580\u001B[0m       create_fn\u001B[38;5;241m=\u001B[39mcreate_fn, init_op_fn\u001B[38;5;241m=\u001B[39minit_op_fn, mesh\u001B[38;5;241m=\u001B[39mexperimental_mesh\n\u001B[0;32m    581\u001B[0m   )\n\u001B[0;32m    582\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 583\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ResourceSummaryWriter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    584\u001B[0m \u001B[43m      \u001B[49m\u001B[43mcreate_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcreate_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit_op_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minit_op_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmesh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexperimental_mesh\u001B[49m\n\u001B[0;32m    585\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Documents\\programme\\anaconda3\\envs\\NLP_TD4\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:320\u001B[0m, in \u001B[0;36m_ResourceSummaryWriter.__init__\u001B[1;34m(self, create_fn, init_op_fn, mesh)\u001B[0m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    319\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_resource \u001B[38;5;241m=\u001B[39m create_fn()\n\u001B[1;32m--> 320\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_op \u001B[38;5;241m=\u001B[39m \u001B[43minit_op_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_resource\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    322\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n",
      "File \u001B[1;32mD:\\Documents\\programme\\anaconda3\\envs\\NLP_TD4\\lib\\site-packages\\tensorflow\\python\\ops\\gen_summary_ops.py:147\u001B[0m, in \u001B[0;36mcreate_summary_file_writer\u001B[1;34m(writer, logdir, max_queue, flush_millis, filename_suffix, name)\u001B[0m\n\u001B[0;32m    145\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m--> 147\u001B[0m   \u001B[43m_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_from_not_ok_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_FallbackException:\n\u001B[0;32m    149\u001B[0m   \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Documents\\programme\\anaconda3\\envs\\NLP_TD4\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5883\u001B[0m, in \u001B[0;36mraise_from_not_ok_status\u001B[1;34m(e, name)\u001B[0m\n\u001B[0;32m   5881\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mraise_from_not_ok_status\u001B[39m(e, name) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NoReturn:\n\u001B[0;32m   5882\u001B[0m   e\u001B[38;5;241m.\u001B[39mmessage \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m name: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(name \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m-> 5883\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mFailedPreconditionError\u001B[0m: {{function_node __wrapped__CreateSummaryFileWriter_device_/job:localhost/replica:0/task:0/device:CPU:0}} D:\\Documents\\école\\NLP_TD4\\logs is not a directory [Op:CreateSummaryFileWriter] name: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(tokenized_data, model, epochs=3, batch_size=8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eac376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T15:57:31.931758500Z",
     "start_time": "2024-02-12T15:57:31.917759600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example of extracting entities\n",
    "test_sentence = \"Send a message to John telling him about the meeting tomorrow.\"\n",
    "extracted_entities = extract_entities(test_sentence, model, tokenizer, label_map_inv={\"O\": 0, \"person\": 1, \"content\": 2}) # Define label_map_inv according to your labels\n",
    "\n",
    "print(extracted_entities)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41455f2c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-12T15:57:31.920760Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "save_model_path = \"./trained_model\" # Update this path as needed\n",
    "save_model(model, save_model_path)\n",
    "\n",
    "print(f\"Model saved to {save_model_path}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

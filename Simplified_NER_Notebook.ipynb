{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476b1201",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T16:09:05.422889700Z",
     "start_time": "2024-02-12T16:09:05.339595400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary functions from ner_utils_simplified.py\n",
    "from src.ner_utils import  load_data_from_csv, create_dataset, tokenize_and_align_labels, train_model, save_model\n",
    "\n",
    "# Additional imports if needed\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForTokenClassification\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adaeae08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T16:09:05.498898Z",
     "start_time": "2024-02-12T16:09:05.347593100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load and prepare data\n",
    "file_path = \"data/train_2.csv\" # Update this path according to your dataset\n",
    "df = load_data_from_csv(file_path)\n",
    "data = create_dataset(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "485a7b76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T16:09:05.750882900Z",
     "start_time": "2024-02-12T16:09:05.501898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2931 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56e2c3b0dfcd475e8fe2f3460c4a2253"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Tokenize and align labels\n",
    "tokenized_data = data.map(tokenize_and_align_labels, batched=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9f0981f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T16:09:07.393539200Z",
     "start_time": "2024-02-12T16:09:05.746309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Documents\\programme\\anaconda3\\envs\\NLP_TD4\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForTokenClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model\n",
    "model_name = 'distilbert-base-uncased'\n",
    "num_labels = 3  # Update this according to your dataset's needs\n",
    "model = TFDistilBertForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6950cf23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-12T16:09:08.470746500Z",
     "start_time": "2024-02-12T16:09:07.389536400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\programme\\anaconda3\\envs\\NLP_TD4\\lib\\site-packages\\transformers\\trainer_tf.py:118: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "{{function_node __wrapped__CreateSummaryFileWriter_device_/job:localhost/replica:0/task:0/device:CPU:0}} ./results is not a directory [Op:CreateSummaryFileWriter] name: ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFailedPreconditionError\u001B[0m                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenized_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Documents\\Ã©cole\\NLP_TD4\\src\\ner_utils.py:117\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(tokenized_data, model, epochs, batch_size)\u001B[0m\n\u001B[0;32m    106\u001B[0m     os\u001B[38;5;241m.\u001B[39mmakedirs(output_dir, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    108\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TFTrainingArguments(\n\u001B[0;32m    109\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39moutput_dir,  \u001B[38;5;66;03m# Output directory\u001B[39;00m\n\u001B[0;32m    110\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39mepochs,  \u001B[38;5;66;03m# Total number of training epochs\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    114\u001B[0m     weight_decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m,  \u001B[38;5;66;03m# Strength of weight decay\u001B[39;00m\n\u001B[0;32m    115\u001B[0m )\n\u001B[1;32m--> 117\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTFTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# The instantiated ðŸ¤— Transformers model to be trained\u001B[39;49;00m\n\u001B[0;32m    119\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Training arguments, defined above\u001B[39;49;00m\n\u001B[0;32m    120\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenized_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Training dataset\u001B[39;49;00m\n\u001B[0;32m    121\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    123\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32mD:\\Documents\\programme\\anaconda3\\envs\\NLP_TD4\\lib\\site-packages\\transformers\\trainer_tf.py:129\u001B[0m, in \u001B[0;36mTFTrainer.__init__\u001B[1;34m(self, model, args, train_dataset, eval_dataset, compute_metrics, tb_writer, optimizers)\u001B[0m\n\u001B[0;32m    127\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtb_writer \u001B[38;5;241m=\u001B[39m tb_writer\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 129\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtb_writer \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msummary\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_file_writer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogging_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_wandb_available():\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetup_wandb()\n",
      "File \u001B[1;32mD:\\Documents\\programme\\anaconda3\\envs\\NLP_TD4\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:583\u001B[0m, in \u001B[0;36mcreate_file_writer_v2\u001B[1;34m(logdir, max_queue, flush_millis, filename_suffix, name, experimental_trackable, experimental_mesh)\u001B[0m\n\u001B[0;32m    579\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _TrackableResourceSummaryWriter(\n\u001B[0;32m    580\u001B[0m       create_fn\u001B[38;5;241m=\u001B[39mcreate_fn, init_op_fn\u001B[38;5;241m=\u001B[39minit_op_fn, mesh\u001B[38;5;241m=\u001B[39mexperimental_mesh\n\u001B[0;32m    581\u001B[0m   )\n\u001B[0;32m    582\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 583\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ResourceSummaryWriter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    584\u001B[0m \u001B[43m      \u001B[49m\u001B[43mcreate_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcreate_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit_op_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minit_op_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmesh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexperimental_mesh\u001B[49m\n\u001B[0;32m    585\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Documents\\programme\\anaconda3\\envs\\NLP_TD4\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:320\u001B[0m, in \u001B[0;36m_ResourceSummaryWriter.__init__\u001B[1;34m(self, create_fn, init_op_fn, mesh)\u001B[0m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    319\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_resource \u001B[38;5;241m=\u001B[39m create_fn()\n\u001B[1;32m--> 320\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_op \u001B[38;5;241m=\u001B[39m \u001B[43minit_op_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_resource\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    322\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n",
      "File \u001B[1;32mD:\\Documents\\programme\\anaconda3\\envs\\NLP_TD4\\lib\\site-packages\\tensorflow\\python\\ops\\gen_summary_ops.py:147\u001B[0m, in \u001B[0;36mcreate_summary_file_writer\u001B[1;34m(writer, logdir, max_queue, flush_millis, filename_suffix, name)\u001B[0m\n\u001B[0;32m    145\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m--> 147\u001B[0m   \u001B[43m_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_from_not_ok_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_FallbackException:\n\u001B[0;32m    149\u001B[0m   \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Documents\\programme\\anaconda3\\envs\\NLP_TD4\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5883\u001B[0m, in \u001B[0;36mraise_from_not_ok_status\u001B[1;34m(e, name)\u001B[0m\n\u001B[0;32m   5881\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mraise_from_not_ok_status\u001B[39m(e, name) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NoReturn:\n\u001B[0;32m   5882\u001B[0m   e\u001B[38;5;241m.\u001B[39mmessage \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m name: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(name \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m-> 5883\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mFailedPreconditionError\u001B[0m: {{function_node __wrapped__CreateSummaryFileWriter_device_/job:localhost/replica:0/task:0/device:CPU:0}} ./results is not a directory [Op:CreateSummaryFileWriter] name: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "train_model(tokenized_data, model, epochs=3, batch_size=8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d23c44",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-12T16:09:08.469237300Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model\n",
    "save_model_path = \"./trained_model\" # Update this path as needed\n",
    "save_model(model, save_model_path)\n",
    "\n",
    "print(f\"Model saved to {save_model_path}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
